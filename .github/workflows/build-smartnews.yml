name: Fetch and Clean RSS Feed

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: |
          pip install lxml requests beautifulsoup4
      
      - name: Fetch and Clean RSS Feed
        run: |
          python3 << 'EOF'
          import requests
          from lxml import etree
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone
          import time
          import re

          print("Fetching RSS feed...")
          response = requests.get('https://www.cabletv.com/feed')
          root = etree.fromstring(response.content)
          channel = root.find('channel')
          items = channel.findall('item')[:8]
          
          print(f"Found {len(items)} items")
          
          fallback_img = 'https://ctv-clearlink.github.io/RSS-Feed/CableTV.com%20RSS%20Logo%20Header.png'
          
          # Start building XML manually
          output = ['<?xml version="1.0" encoding="UTF-8"?>']
          output.append('<rss version="2.0"')
          output.append(' xmlns:content="http://purl.org/rss/1.0/modules/content/"')
          output.append(' xmlns:dc="http://purl.org/dc/elements/1.1/"')
          output.append(' xmlns:media="http://search.yahoo.com/mrss/"')
          output.append(' xmlns:snf="http://www.smartnews.be/snf">')
          output.append('<channel>')
          output.append(f'<snf:logo><url>{fallback_img}</url></snf:logo>')
          
          # Copy channel metadata
          for elem in channel:
              if elem.tag == 'item':
                  break
              if 'logo' not in str(elem.tag).lower():
                  output.append(etree.tostring(elem, encoding='unicode'))
          
          # Process each item
          for idx, item in enumerate(items, 1):
              link_elem = item.find('link')
              if link_elem is None or link_elem.text is None:
                  continue
                  
              link = link_elem.text.strip().split('?')[0]
              print(f"\n[{idx}] {link}")
              
              # Get RSS fallbacks
              rss_title = item.find('title')
              title = rss_title.text if rss_title is not None else "Unknown"
              
              image = fallback_img
              content = ""
              
              # Get RSS content as fallback
              for elem in item:
                  if 'encoded' in str(elem.tag):
                      content = elem.text if elem.text else ""
                      break
              
              # Scrape the page
              try:
                  page = requests.get(link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}, timeout=15)
                  
                  if page.status_code == 200:
                      html = page.text
                      
                      # Extract og:title using REGEX (more reliable than BeautifulSoup for meta tags)
                      title_match = re.search(r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                      if not title_match:
                          # Try reverse order (content before property)
                          title_match = re.search(r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+property=["\']og:title["\']', html, re.IGNORECASE)
                      
                      if title_match:
                          title = title_match.group(1)
                          # Decode HTML entities
                          title = title.replace('&amp;', '&').replace('&#8217;', "'").replace('&#8211;', '–').replace('&#8212;', '—')
                          print(f"  ✓ Title: {title}")
                      else:
                          print(f"  ✗ No og:title found, using RSS: {title}")
                      
                      # Extract og:image using REGEX
                      img_match = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                      if not img_match:
                          img_match = re.search(r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+property=["\']og:image["\']', html, re.IGNORECASE)
                      
                      if img_match:
                          img_url = img_match.group(1)
                          if 'logo' not in img_url.lower() and 'icon' not in img_url.lower():
                              image = img_url
                              print(f"  ✓ Image: .../{img_url.split('/')[-1]}")
                      
                      # Extract article content using BeautifulSoup
                      soup = BeautifulSoup(html, 'html.parser')
                      article = soup.find('article')
                      if article:
                          # Remove unwanted elements
                          for unwanted in article.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                              unwanted.decompose()
                          
                          # Remove forms
                          for form in article.find_all('form'):
                              form.decompose()
                          
                          # Convert to string
                          content = str(article)
                          print(f"  ✓ Content: {len(content):,} characters")
                      else:
                          print(f"  ✗ No article tag, using RSS content ({len(content)} chars)")
                  else:
                      print(f"  ✗ HTTP {page.status_code}")
                  
                  time.sleep(0.5)
                  
              except Exception as e:
                  print(f"  ✗ Error: {str(e)[:100]}")
              
              # Build item XML
              output.append('<item>')
              output.append(f'<title><![CDATA[{title}]]></title>')
              
              # Copy other RSS elements (skip title, content:encoded, media:thumbnail)
              for elem in item:
                  tag_str = str(elem.tag)
                  if elem.tag == 'title':
                      continue
                  if 'encoded' in tag_str or 'thumbnail' in tag_str:
                      continue
                  output.append(etree.tostring(elem, encoding='unicode'))
              
              # Add content
              output.append(f'<content:encoded><![CDATA[{content}]]></content:encoded>')
              
              # Add media thumbnail
              safe_image = image.replace('&', '&amp;')
              output.append(f'<media:thumbnail url="{safe_image}"/>')
              
              output.append('</item>')
          
          output.append('</channel>')
          output.append('</rss>')
          
          final_xml = ''.join(output)
          final_xml = final_xml.replace('https://www.cabletv.com/feed', 'https://ctv-clearlink.github.io/RSS-Feed/feed.xml')
          
          # Write to file
          with open('feed.xml', 'w', encoding='utf-8') as f:
              f.write(final_xml)
          
          print(f"\n✅ Feed created: {len(final_xml):,} bytes")
          EOF
      
      - name: Check for changes
        id: changes
        run: |
          git diff --quiet feed.xml || echo "changed=true" >> $GITHUB_OUTPUT
      
      - name: Commit and push feed
        if: steps.changes.outputs.changed == 'true'
        run: |
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git config user.name "github-actions[bot]"
          git add feed.xml
          git commit -m "Update RSS feed"
          git pull --rebase origin main
          git push origin main
